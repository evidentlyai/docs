---
title: 'Report'
description: 'How to generate Report.'
---

Reports perform evaluations on the Dataset level and/or summarize results of the row-level evaluations. For a general introduction, check [Core Concepts](/docs/library/overview).

**Pre-requisites**:

* You [installed Evidently](/docs/setup/installation).

* You created a Dataset with the [Data Definition](/docs/library/data_definition).

* (Optional) for text data, you added Descriptors.

<Note>
  For a quick end-to-end example of generating Reports, Check the Quickstart [for ML](/quickstart_ml) or [LLM](/quickstart_llm).
</Note>

## Imports

Import the Metrics and Presets you plan to use.

```python
from evidently import Report
from evidently.metrics import *
from evidently.presets import *
```

You can use Metric Presets, which are pre-built Reports that work out of the box, or create a custom Report selecting Metrics one by one.

## Presets

<Tip>
  **Available Presets**. Check available evals in the [Reference table](/metrics/all_metrics).
</Tip>

To generate a template Report, simply pass the selected Preset to the Report.

**Single dataset**. To generate the Data Summary Report for a single dataset:

```python
report = Report([
    DataSummaryPreset()
])

my_eval = report.run(eval_data_1, None)
my_eval
#my_eval.json
```

**Two datasets**. To generate the Data Drift Report that needs two datasets, pass the second one as a reference when you `run` it:

```python
report = Report([
    DataDriftPreset()
])

my_eval = report.run(eval_data_1, eval_data_2)
my_eval
#my_eval.json
```

<Note>
  Note that in this case the order matters: the first `eval_data_1` is the current data you evaluate, the second `eval_data_2` is the reference dataset you consider as a baseline for drift detection.
</Note>

If nothing else is specified, the Report will run with the default parameters for all columns in the dataset. You can also pass custom parameters to some Presets.

**Combine Presets**. You can also include multiple Presets in the same Report. List the m one by one.

```python
report = Report([
    DataDriftPreset(), 
    DataSummaryPreset()
])

my_eval = report.run(eval_data_1, eval_data_2)
my_eval
#my_eval.json
```

**Limit columns**. You can limit the columns to which the Preset is applied.

```python
report = Report([
    DataDriftPreset(column=["target", "prediction"])
])

my_eval = report.run(eval_data_1, eval_data_2)
my_eval
#my_eval.json
```

You can view the Report in Python, export the outputs (HTML, JSON, Python dictionary) or upload it to the Evidently platform. Check more in [output formats](/docs/library/output_formats).

## Custom Report

<Tip>
  **Available Metrics and parameters**. Check available evals in the [Reference table](/metrics/all_metrics).
</Tip>

**Custom Report**. To create a custom Report, simply list the Metics one by one. You can combine both dataset-level and column-level Metrics, and combine Reports and Metrics in one Report. When you use a column-level Metric, you must specify the column it refers to.

```python
report = Report([
    ColumnCount(), 
    ValueStats(column="target")
])

my_eval = report.run(eval_data_1, None)
my_eval
#my_eval.json
```

<Note>
  **Generating multiple column-level Metrics**: You can use a helper function to easily generate multiple column-level Metrics for a list of columns. See the page on [Metric Generator](/docs/library/metric_generator).
</Note>

**Metric Parameters**. Metrics can have optional or required parameters.

For example, the data drift detection algorithm automatically selects a method, but you can override this by specifying your preferred method (Optional).

```python
report = Report([
   ValueDrift(column="target", method="psi")
])
```

To calculate the Precision at K for a ranking task, you must always pass the `k` parameter (Required).

```python
report = Report([
   PrecisionTopK(k=10)
])
```

## Group by

You can calculate metrics separately for different groups in your data, using a column with categories to split by. Use the `GroupyBy` metric as shown below.

**Example**. This will compute the maximum value of salaries by each label in the "Department" column.

```python
from evidently.future.metrics.group_by import GroupBy

report = Report([
    GroupBy(MaxValue(column="Salary"), "Department"),
])
my_eval = report.run(data, None)
my_eval.dict()
```

Note: you cannot use auto-generated Test conditions when you use GroupBy.

## What's next?

You can also add conditions to Metrics: check the [Tests guide](/docs/library/tests).