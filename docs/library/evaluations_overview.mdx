---
title: 'Overview'
description: 'End-to-end evaluation workflow.'
---

This page shows the core eval workflow with the Evidently library and links to guides.

## Define and run the eval

<Tip>
  To log the evaluation results to the Evidently Platform, first connect to [Evidently Cloud](/docs/setup/cloud) or your [local workspace](/docs/setup/self-hosting) and [create a Project](/docs/platform/projects_manage). It's optional: you can also run evals locally.
</Tip>

<Steps>
  <Step title="Prepare the input data">
    Get your data in a table like a `pandas.DataFrame`. More on [data requirements](/docs/library/overview#dataset). You can also [load data](/docs/platform/datasets_workflow) from Evidently Platform, like tracing or synthetic datasets.
  </Step>

  <Step title="Create a Dataset object">
    Create a Dataset object with `DataDefinition()` that specifies column role and types. You can also use default type detection. [How to set Data Definition](/docs/library/data_definition).

    ```python
    eval_data = Dataset.from_pandas(
        pd.DataFrame(source_df),
        data_definition=DataDefinition()
    )
    ```
  </Step>

  <Step title="(Optional) Add descriptors">
    For text evals, choose and compute row-level `descriptors`. [How to use Descriptors](/docs/library/descriptors).

    ```python
    eval_data.add_descriptors(descriptors=[
        TextLength("Question", alias="Length"),
        Sentiment("Answer", alias="Sentiment")
    ])
    ```
  </Step>

  <Step title="Configure Report">
    For dataset-level evals (classification, data drift) or to summarize descriptors, create a `Report` with chosen `metrics`  or `presets`. How to [configure Reports](/docs/library/report).

    ```python
    report = Report([
        DataSummaryPreset()
    ])
    ```
  </Step>

  <Step title="(Optional) Add Test conditions">
    Add Pass/Fail conditions, like to check if text length in \< 100 symbols. How to [configure Tests](/docs/library/tests).

    ```python
    report = Report([
        DataSummaryPreset(),
        MaxValue(column="Length", tests=[lt(100)]),
    ])
    ```
  </Step>

  <Step title="(Optional) Add Tags and Timestamps">
    Add `tags` or `metadata` to identify specific evaluation runs or datasets, or override the default `timestamp `. [How to add metadata](/docs/library/tags_metadata).
  </Step>

  <Step title="Run the Report">
    To execute the eval, `run`the Report on the `Dataset` (or two).

    ```python
    my_eval = report.run(eval_data, None)
    ```
  </Step>

  <Step title="Explore the results">
    * To upload to the Evidently Platform. [How to upload results](/docs/platform/evals_api).

    ```python
    ws.add_report(project.id, my_eval, include_data=True)
    ```

    * To view locally. [All output formats](/docs/library/output_formats).

    ```python
    my_eval
    ##my_eval.json()
    ```
  </Step>
</Steps>

## Quickstarts

Check for end-to-end examples:

<CardGroup cols={2}>
  <Card title="LLM quickstart" icon="comment-text" href="/quickstart_llm">
    Evaluate the quality of text outputs.
  </Card>

  <Card title="ML quickstart" icon="table" href="/quickstart_ml">
    Test tabular data quality and data drift.
  </Card>
</CardGroup>