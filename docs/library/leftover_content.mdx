---
title: 'Leftovers'
description: 'Description of your new file.'
noindex: true
---

Use for relevant pages after features are implemented.

# Metrics

## Correlations

Use for exploratory data analysis, drift monitoring (correlation changes) or to check alignment between scores (e.g. LLM-based descriptors against human labels).

<Info>
  [Data definition](/docs/library/data_definition). You may need to map column types.
</Info>

Column data quality

| Metric                                  | Description                                                                                                                                                                                               | Parameters                                                                                                                                                                                                                        | Test Defaults                                                                                  |
| --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------
| **RowsWithMissingValuesCount()**  (Coming soon) | <ul><li> Dataset-level.</li><li>Counts rows with missing values.</li><li>Metric result: `value`.</li></ul>                                                                             | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one row with missing values.</li><li>**With reference**: Fails if share differs by >10% (+/-)</li></ul>           |
| **AlmostEmptyColumnCount()**  (Coming soon)     | <ul><li> Dataset-level.</li><li>Counts almost empty columns (95% empty).</li><li>Metric result: `value`.</li></ul>                                                                     | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one almost empty column.</li><li>**With reference**: Fails if count is higher than in reference.</li></ul>        |
| **NewCategoriesCount()** (Coming soon)                                                                                         | <ul><li>Column-level.</li><li>Counts new categories compared to reference (reference required).</li><li>Metric result: `count`, `share`.</li></ul>   | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                | Expect 0.                                                                                                                                                          |
| **MissingCategoriesCount()**  (Coming soon)                                                                                    | <ul><li>Column-level.</li><li>Counts missing categories compared to reference.</li><li>Metric result: `count`, `share`.</li></ul>                    | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                | Expect 0.                                                                                                                                                          |
| **MostCommonValueCount()** (Coming soon)                                                                                       | <ul><li>Column-level.</li><li>Identifies the most common value and provides its count/share.</li><li>Metric result: `value: count, share`.</li></ul> | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                | <ul><li>**No reference**: Fails if most common value share is ≥80%.</li><li>**With reference**:  Fails if most common value share differs by >10% (+/-).</li></ul> |

Drift

| Metric                                  | Description                                                                                                                                                                                               | Parameters                                                                                                                                                                                                                        | Test Defaults                                                                                  |
| --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------- 
| **EmbeddingDrift()** (Coming soon)    | <ul><li>Column-level.</li><li> Requires reference.</li><li>Calculates data drift for embeddings.</li><li>Requires embedding columns set in data definition.</li><li>Metric result: `value`.</li></ul>                                                                                                                                      | **Required**: <ul><li>`embeddings`</li><li>`method`</li></ul> See [embedding drift options](/metrics/customize_embedding_drift).                                                                                                                                                       | <ul><li>**With reference**: Defaults for method. See [methods](/metrics/customize_embedding_drift).</li></ul>                            |
| **MultivariateDrift()** (Coming soon) | <ul><li>Dataset-level.</li><li> Requires reference.</li><li>Computes a single dataset drift score.</li><li>Default method: share of drifted columns.</li><li>Metric result: `value`.</li></ul>                                                                                                                                             | **Optional**: <ul><li>`columns`</li><li>`method`</li></ul>See [drift options](/metrics/customize_data_drift).                                                                                                                                                                          | <ul><li>**With reference**: Defaults for method. See [methods](/metrics/customize_data_drift).             </li></ul>                    |

| Metric                                  | Description                                                                                                                                                                                               | Parameters                                                                                                                                                                                                                        | Test Defaults                                                                                  |
| --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------- |
| **DatasetCorrelations()** (Coming soon) | <ul><li>Calculates the correlations between all or set columns in the dataset.</li><li>Supported methods: Pearson, Spearman, Kendall, Cramer\_V.</li></ul>                                                | **Optional**: <ul><li>`columns`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                                                                          | N/A                                                                                            |
| **Correlation()** (Coming soon)         | <ul><li>Calculates the correlation between two defined columns.</li></ul>                                                                                                                                 | **Required**: <ul><li>`column_x`</li><li>`column_y`</li></ul>**Optional**:<ul><li>`method` (default: `pearson`, available: `pearson`, `spearman`, `kendall`, `cramer_v`)</li><li>[Test conditions](/docs/library/tests)</li></ul> | N/A                                                                                            |
| **CorrelationChanges()** (Coming soon)  | <ul><li>Dataset-level.</li><li>Reference required.</li><li>Checks the number of correlation violations (significant changes in correlation strength between columns) across all or set columns.</li></ul> | **Optional**: <ul><li>`columns`</li><li>`method` (default: `pearson`, available: `pearson`, `spearman`, `kendall`, `cramer_v`)</li><li>`corr_diff` (default: 0.25)</li><li>[Test conditions](/docs/library/tests)</li></ul>       | <ul><li>**With reference**: Fails if at least one correlation violation is detected.</li></ul> |


Classification

| Metric                              | Description                                                                                                                                                                         | Parameters                                                                                                                                                     | Test Defaults                                                                                                                           |
| ----------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |
| **LabelCount()** (Coming soon) | <ul><li>Distribution of predicted classes.</li><li>Can visualize class balance and/or probability distribution.</li></ul>                                                   | **Required**: <ul><li>Set at least one visualization: `class_balance`, `prob_distribution`.</li></ul>  **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                                    | N/A                                                                                                                                                                                  |
| **Lift()**  (Coming soon)      | <ul><li>Calculates lift.</li><li>Can visualize lift curve or table.</li><li>Metric result: `value`.</li></ul>                                                               | **Required**: <ul><li>Set at least one visualization: `lift_table`, `lift_curve`.</li></ul> **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                    | N/A                                                                                                                                                                                  |

Recsys

<Info>
  [Data definition](/docs/library/data_definition). You may need to map prediction and target columns and ranking type. Some metrics require additional training data.
</Info>

| Metric                              | Description                                                                                                                                                                         | Parameters                                                                                                                                                     | Test Defaults                                                                                                                           |
| ----------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |
| **Personalization()** (Coming soon) | <ul><li>Calculates Personalization score at the top K recommendations.</li><li>Metric result: `value`.</li></ul>                                                                    | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                     | <ul><li>**No reference**: Tests if Personalization > 0.</li><li>**With reference**: Fails if Personalization differs by >10%.</li></ul> |
| **ARP()** (Coming soon)             | <ul><li>Computes Average Recommendation Popularity at the top K recommendations.</li><li>Requires a training dataset.</li><li>Metric result: `value`.</li></ul>                     | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`normalize_arp` (default: `False`)</li><li>[Test conditions](/docs/library/tests)</li></ul>          | <ul><li>**No reference**: Tests if ARP > 0.</li><li>**With reference**: Fails if ARP differs by >10%.</li></ul>                         |
| **Coverage()**(Coming soon)         | <ul><li>Calculates Coverage at the top K recommendations.</li><li>Requires a training dataset.</li><li>Metric result: `value`.</li></ul>                                            | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                     | <ul><li>**No reference**: Tests if Coverage > 0.</li><li>**With reference**: Fails if Coverage differs by >10%.</li></ul>               |
| **GiniIndex()**(Coming soon)        | <ul><li>Calculates Gini Index at the top K recommendations.</li><li>Requires a training dataset.</li><li>Metric result: `value`.</li></ul>                                          | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                     | <ul><li>**No reference**: Tests if Gini Index \< 1.</li><li>**With reference**: Fails if Gini Index differs by >10%.</li></ul>          |
| **Diversity()**  (Coming soon)      | <ul><li>Calculates Diversity at the top K recommendations.</li><li>Requires item features.</li><li>Metric result: `value`.</li></ul>                                                | **Required**: <ul><li>`k`</li><li>`item_features`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                             | <ul><li>**No reference**: Tests if Diversity > 0.</li><li>**With reference**: Fails if Diversity differs by >10%.</li></ul>             |
| **Serendipity()**(Coming soon)      | <ul><li>Calculates Serendipity at the top K recommendations.</li><li>Requires a training dataset.</li><li>Metric result: `value`.</li></ul>                                         | **Required**: <ul><li>`k`</li><li>`item_features`</li></ul> **Optional**: <ul><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul>     | <ul><li>**No reference**: Tests if Serendipity > 0.</li><li>**With reference**: Fails if Serendipity differs by >10%.</li></ul>         |
| **Novelty()**  (Coming soon)        | <ul><li>Calculates Novelty at the top K recommendations.</li><li>Requires a training dataset.</li><li>Metric result: `value`.</li></ul>                                             | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                     | <ul><li>**No reference**: Tests if Novelty > 0.</li><li>**With reference**: Fails if Novelty differs by >10%.</li></ul>                 |

Relevant for RecSys metrics:

* `no_feedback_user: bool = False`. Specifies whether to include the users who did not select any of the items, when computing the quality metric. Default: False.

* `min_rel_score: Optional[int] = None`. Specifies the minimum relevance score to consider relevant when calculating the quality metrics for non-binary targets (e.g., if a target is a rating or a custom score).

# Ranking metrics explainers

### Diversity

**Evidently Metric**: `Diversity`

**Recommendation diversity**: this metric measures the average intra-list diversity at K. It reflects the variety of items within the same user's recommendation list, averaged by all users. 

**Implemented method**:
* **Measure the difference between recommended items**. Calculate the Cosine distance for each pair of recommendations inside the top-K in each user's list. The cosine distance serves as a measure of diversity between vectors representing recommended items, and is computed as:

$$\text{Cosine distance} = 1 - \text{Cosine Similarity}$$

Link: [Cosine Similarity on Wikipedia](https://en.wikipedia.org/wiki/Cosine_similarity). 

* **Intra-list diversity**. Calculate intra-list diversity for each user by averaging the Cosine Distance between each pair of items in the user's top-K list.
* **Overall diversity**. Calculate the overall diversity by averaging the intra-list diversity across all users.

**Range**: The metric is based on Cosine distance, and can take values from 0 to 2. 
**0:** identical recommendations in top-K.
**2:** very diverse recommendations in top-K.

**Interpretation**: the higher the value, the more varied items are shown to each user (e.g. inside a single recommendation block).

**Requirements**: You must pass the `item_features` list to point to numerical columns or embeddings that describe the recommended items. For example, these could be encoded genres that represent each movie. This makes it possible to compare the degree of similarity between different items. 

**Notes**: 
* This metric does not consider relevance. A recommender system showing varied but irrelevant items will have high diversity.
* This method performs many pairwise calculations between items and can take some time to compute.
  
### Novelty

**Evidently Metric**: `Novelty`

**Recommendation novelty**: this metric measures the average novelty of recommendations at K. It reflects how unusual top-K items are shown to each user, averaged by all users. 

**Implemented method**:
* Measure **novelty of recommended items**. The novelty of an item can be defined based on its popularity in the training set.

$$\text{novelty}_i = -\log_2(p_i)$$
where *p* represents the probability that item *i* is observed. It is calculated as the share of users that interacted with an item in the training set.

$$\text{novelty}_i = -\log_2\left(\frac{\text{users who interacted with } i}{\text{number of users}}\right)$$

High novelty corresponds to long-tail items that few users interacted with, and low novelty values correspond to popular items. If all users had interacted with an item, novelty is 0.
* Measure **novelty by user**. For each user, compute the average item novelty at K, by summing up the novelty of all items and dividing by K.
* **Overall novelty**. Average the novelty by user across all users.

**Range**: 0 to infinity. 

**Interpretation**: if the value is higher, the items shown to users are more unusual. If the value is lower, the recommended items are well-known.   

**Notes**: 
* This metric does not consider relevance. A recommender system showing many irrelevant but unexpected (long tail) items will have high novelty. 
* It is not possible to define the novelty of an item absent in the training set. The evaluation only considers items that are present in training. 

Further reading: [Castells, P., Vargas, S., & Wang, J. (2011). Novelty and Diversity Metrics for Recommender Systems: Choice, Discovery and Relevance](https://repositorio.uam.es/bitstream/handle/10486/666094/novelty_castells_DDR_2011.pdf)

### Serendipity

**Evidently Metric**: `Serendipity`

Recommendation serendipity: this metric measures how unusual the relevant recommendations are in K, averaged for all users. 

Serendipity combines unexpectedness and relevance. It reflects the ability of a recommender system to show relevant items (that get a positive ranking or action) that are unexpected in the context of the user history (= are not similar to previous interactions). For example, a user who usually likes comedies gets recommended and upvotes a thriller.

**Implemented method**. 
* Measure the **unexpectedness** of relevant recommendations. The “unexpectedness” is measured using Cosine distance. For every relevant recommendation in top-K, we compute the distance between this item and the previous user interactions in the training set. Higher cosine distance indicates higher unexpectedness.

$$\text{serendipity}_i = \text{unexpectedness}_i\times\text{relevance}_i$$

Where *relevance(i)* is equal to 1 if the item is relevant, and is 0 otherwise.
* **Serendipity by user**. Calculate the average of the resulting distances for all relevant recommendations in the user list.  
* **Overall serendipity**. Calculate the overall recommendation serendipity by averaging the results across all users.

$$\text{Serendipity} = 1 - \sum_{u \in S} \frac{1}{|S| |H_u|} \sum_{h \in H_u} \sum_{i \in R_{u,k}} \frac{\text{CosSim}(i, h)}{k}$$

Where
* *S* is the set of all users.
* *H(u)* is the item history of user *u*.
* *R(u)* Top-K function, where *R(u,k)* gives the top *k* recommended items for user *u*.

**Range**: The metric is based on Cosine distance, and can take values from 0 to 2. 
* **0**: only popular, expected relevant recommendations.
* **2**: completely unexpected relevant recommendations.
 
**Interpretation**: the higher the value, the better the ability of the system to “positively surprise” the user. 

**Requirements**: You must pass the `item_features` list to point to the numerical columns or embeddings that describe the recommended items. This allows comparing the degree of similarity between recommended items.

**Notes**: 
* This metric is only computed for the users that are present in the training set. If there is no previous recommendation history, these users will be ignored. 
* This metric only considers the unexpectedness of relevant items in top-K. Irrelevant recommendations, and their share, are not taken into account.

Further reading: [Zhang, Y., Séaghdha, D., Quercia, D., Jambor, T. (2011). Auralist: introducing serendipity into music recommendation.](http://www.cs.ucl.ac.uk/fileadmin/UCL-CS/research/Research_Notes/RN_11_21.pdf)

### Personalization

**Evidently Metric**: `Personalization`

Personalization of recommendations: this metric measures the average uniqueness of each user's recommendations in top-K.

**Implemented method**:
* For every two users, compute the **overlap between top-K recommended items**. (The number of common items in top-K between two lists, divided by K).
* Calculate the **average overlap** across all pairs of users.
* Calculate personalization as: 

$$\text{Personalization} = 1 - \text{average overlap}$$

The resulting metric reflects the average share of unique recommendations in each user’s list.

**Range**: 0 to 1.
* **0**: Identical recommendations for each user in top-K. 
* **1**: Each user’s recommendations in top-K are unique.   

**Interpretation**: the higher the value, the more personalized (= different from others) is each user’s list. The metric visualization also shows the top-10 most popular items.

### Average Recommendation Popularity (ARP)

**Evidently Metric**: `ARP`

The recommendation popularity bias is a tendency to favor a few popular items. 

ARP reflects the average popularity of the items recommended to the users. 

**Implementation**.
* Compute the item popularity as the number of times each item was seen in training. 
* Compute the average popularity for each user’s list as a sum of all items’ popularity divided by the number of recommended items.
* Compute the average popularity for all users by averaging the results across all users.

$$ARP = \frac{1}{|U|} \sum_{u \in U} \frac{1}{|L_u|} \sum_{i \in L_u} \phi(i)$$

Where:
* *U* is the total number of users.
* *L(u)* is the list of items recommended for the user *u*.
* *ϕ(i)* is the number of times item *i* was rated in the training set (popularity of item *i*)

**Range**: 0 to infinity 

**Interpretation**: the higher the value, the more popular on average the recommendations are in top-K.  

**Note**: This metric is not normalized and depends on the number of recommendations in the training set.

Further reading: [Abdollahpouri, H., Mansoury, M., Burke, R., Mobasher, B., & Malthouse, E. (2021). User-centered Evaluation of Popularity Bias in Recommender Systems](https://dl.acm.org/doi/fullHtml/10.1145/3450613.3456821)

### Coverage

**Evidently Metric**: `Coverage`

Coverage reflects the item coverage as a proportion of items that has been recommended by the system.

**Implementation**: compute the share of items recommended to the users out of the total number of potential items (as seen in the training dataset).

$$\text{Coverage} = \frac{\text{Number of unique items recommended} K}{\text{Total number of unique items}}$$

**Range**: 0 to 1, where 1 means that 100% of items have been recommended to users. 

**Interpretation**: the higher the value (usually preferable), the larger the share of items represented in the recommendations. Popularity-based recommenders that only recommend a limited number of popular items will have low coverage.

### Gini index 

**Evidently Metric**: `GiniIndex`

Gini index: reflects the inequality in the distribution of recommended items shown to different users, as compared to a perfectly equal distribution. 

**Implementation**:  

$$ Gini(L) = 1 - \frac{1}{|I| - 1} \sum_{k=1}^{|I|} (2k - |I| - 1) p(i_k | L)$$

Where 
* *L* is the combined list of all recommendation lists given to different users (note that an item may appear multiple times in L, if recommended for more than one user).
* *p(i|L)* is the ratio of occurrence of item *i* in *L*.
* *I* is the set of all items in the catalog.

**Range**: 0 to 1, where 0 represents the perfect equality (recommended items are evenly distributed among users), and 1 is complete inequality (the recommendations are concentrated on a single item).

**Interpretation**: the lower the value (usually preferable), the more equal the item distribution in recommendations. If the value is high, a few items are frequently recommended to many users while others are ignored.

Further reading: [Abdollahpouri, H., Mansoury, M., Burke, R., Mobasher, B., & Malthouse, E. (2021). User-centered Evaluation of Popularity Bias in Recommender Systems](https://dl.acm.org/doi/fullHtml/10.1145/3450613.3456821)
