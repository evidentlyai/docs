---
title: 'Run evals via API'
description: 'How to run evals and log them on the platform'
noindex: true
---

![](/images/evals_flow_python.png)

This relies on the core evaluation API of the Evidently Python library. Check the [detailed guide](/docs/library/evaluations_overview).

## Simple Example

<Tip>
  You must first connect to [Evidently Cloud](/docs/setup/cloud) and [create a Project](/docs/platform/projects_manage).
</Tip>

To run a single eval with text evaluation results uploaded to a workspace:

```
eval_data = Dataset.from_pandas(
    pd.DataFrame(source_df),
    data_definition=DataDefinition()
)

report = Report([
    TextEvals()
])

my_eval = report.run(eval_data, None)
ws.add_report(project.id, my_eval, include_data=True)
```

## Workflow

The complete workflow looks as the following.

<Steps>
  <Step title="Run a Report">
    Configure the evals and run the [Evidently Report](/docs/library/report) with optional [Test ](/docs/library/tests)conditions.
  </Step>

  <Step title="Upload to the platform">
    Upload the raw data or only the evaluation results.
  </Step>

  <Step title="Explore the results">
    Go to the Explore view inside your Project to debug the results and compare the outcomes between runs. Understand the [Explore view](/docs/platform/evals_explore).
  </Step>

  <Step title="(Optional) Set up a Dashboard">
    Set a Dashboard to track results over time. This helps you monitor metric changes across experiments or results of ongoing safety Tests. Check the docs on [Dashboard](/docs/platform/dashboard_overview).
  </Step>

  <Step title="(Optional) Configure alerts">
    Optionally, configure alerts on failed Tests. Check the section on [Alerts](/docs/platform/alerts).
  </Step>
</Steps>

## Uploading data

<Check>
  Raw data upload is available only for Evidently Cloud and Enterprise.
</Check>

When you upload a Report, you can decide to:

* include only the resulting Metrics and a summary Report (with distribution summaries, etc.), or

* also upload the raw Dataset you evaluated, together with added Descriptors if any. This helps with row-level debugging and analysis.

Use`include_data` (default `False`) to specify whether to include the data.

```python
ws.add_report(project.id, my_eval, include_data=False)
```

## Upload existing snapshots

The `add_report` method save Reports in the correct JSON format (`snapshot`). If you already have a snapshot (e.g., you previously saved a Report as a snapshot locally), you can load it load and send it to your Project:

```python
ws.add_snapshot(project.id, snapshot.load("data_drift_snapshot.json"))
```

## Delete snapshots

To delete individual snapshots in the Workspace `ws`, pass the Project ID and snapshot ID. You can see the snapshot ID on the Report page.&#x20;

```python
ws.delete_snapshot(project_id, snapshot_id)
```

You can also delete Reports from the UI.