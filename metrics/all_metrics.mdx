---
title: "All Metrics"
description: "Reference page for all dataset-level evals."
---

<Info>
  For an intro, read [Core Concepts](/docs/library/overview) and check quickstarts for [LLMs](docs/quickstart_llm) or [ML](docs/quickstart_ml).
</Info>

<Accordion title="How to read the tables" defaultOpen={false}>
  * **Metric**: the name of Metric or Preset you can pass to `Report`.

  * **Description:** what it does. Complex Metrics link to explainer pages.

  * **Parameters:** available options. You can also add conditional `tests` to any Metric with standard operators like `eq` (equal), `gt` (greater than), etc. [How Tests work](/docs/library/tests).

  * **Test defaults** are conditions that apply when you invoke Tests but do not set a pass/fail condition yourself.

    * **With reference**: if you provide a reference dataset during the Report `run`, the conditions are set relative to reference.

    * **No reference**: if you do not provide a reference, Tests will use fixed heuristics (like expect no missing values).
</Accordion>

## Text Evals

Summarizes results of text or LLM evals. To score individual inputs, first use [descriptors](/metrics/all_descriptors).

<Info>
  [Data definition](/docs/library/data_definition). You may need to map text columns.
</Info>

| Metric          | Description                                                                                                                                                                                                                                                                                             | Parameters                                 | Test Defaults                          |
| --------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------ | -------------------------------------- |
| **TextEvals()** | <ul><li>Large Preset.</li><li>Shows `ValueStats` for all descriptors.</li><li>You must specify descriptors ([see how](/docs/library/descriptors) and [all descriptors](/metrics/all_descriptors)).</li><li>Metric result: for all Metrics.</li><li>[Preset page](/metrics/preset_text_evals).</li></ul> | **Optional**:  <ul><li>`columns`</li></ul> | As in Metrics included in `ValueStats` |

## Columns

Use to aggregate descriptor results or check data quality on column level.

<Info>
  You may need to map column types using [Data definition](/docs/library/data_definition).
</Info>

### Value stats

Descriptive statistics.

| Metric                                                                                                         | Description                                                                                                                                                                                                                                 | Parameters                                                                                                                                        | Test Defaults                                                                                                                        |
| -------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |
| **ValueStats()**                                                                                               | <ul><li>Small Preset, column-level. </li><li>Computes various descriptive stats. Included Metrics: `UniqueValueCount`, `MissingValueCount`, `MinValue`, `MaxValue`, `MeanValue`, `StdValue`, `QuantileValue` (0.25, 0.5, 0.75).</li><li>Returns different stats based on the column type.</li></ul> | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                   | <ul><li>**No reference**. As in individual Metrics.</li><li>**With reference**. As in indiviudal Metrics.</li></ul>                  |
| **MinValue()**                                                                                                 | <ul><li>Column-level.</li><li>Returns min value for a given numerical column.</li><li>Metric result: `value`.</li></ul>                                                                                                                     | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                   | <ul><li>**No reference**. N/A.</li><li>**With reference**. Fails if Min Value is differs by more than 10% (+/-).</li></ul>           |
| **StdValue()**                                                                                                 | <ul><li>Column-level.</li><li>Computes the standard deviation of a given numerical column.</li><li>Metric result: `value`.</li></ul>                                                                                                        | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                   | <ul><li>**No reference**. N/A.</li><li>**With reference**. Fails if the standard deviation differs by more than 10% (+/-).</li></ul> |
| **MeanValue()**                                                                                                | <ul><li>Column-level.</li><li>Computes the mean value of a given numerical column.</li><li>Metric result: `value`.</li></ul>                                                                                                                | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                   | <ul><li>**No reference**. N/A.</li><li>**With reference**. Fails if the mean value differs by more than 10%.</li></ul>               |
| **MaxValue()**                                                                                                 | <ul><li>Column-level.</li><li>Computes the max value of a given numerical column.</li><li>Metric result: `value`.</li></ul>                                                                                                                 | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                   | <ul><li>**No reference**. N/A.</li><li>**With reference**. Fails if the max value is higher than in the reference.</li></ul>         |
| **MedianValue()**                                                                                              | <ul><li>Column-level.</li><li>Computes the median value of a given numerical column.</li><li>Metric result: `value`.</li></ul>                                                                                                              | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                   | <ul><li>**No reference**. N/A.</li><li>**With reference**. Fails if the median value differs by more than 10% (+/-).</li></ul>       |
| **QuantileValue()**                                                                                            | <ul><li>Column-level.</li><li>Computes the quantile value of a given numerical column.</li><li>Defaults to 0.5 if no quantile is specified.</li><li>Metric result: `value`.</li></ul>                                                       | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>`quantile` (default: 0.5)</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**. N/A.</li><li>**With reference**. Fails if quantile value differs by more than 10% (+/-).</li></ul>         |
| **CategoryCount()** <br /><br /> Example: <br /> `CategoryCount(`<br />`column="city",`<br />` category="NY")` | <ul><li>Column-level.</li><li>Counts occurrences of the specified category or categories. </li><li> To check the joint share of several categories, pass the list `categories=["a", "b"]`.</li><li>Metric result: `count`, `share`.</li></ul>                                                                                                              | **Required**: <ul><li>`column`</li><li>`category`</li><li>`categories`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                | <ul><li>**No reference**. N/A.</li><li>**With reference**. Fails if the specified category is not present.</li></ul>                 |

### Column data quality

Column-level data quality metrics.

<Info>
  [Data definition](/docs/library/data_definition). You may need to map column types.
</Info>

| Metric                                                                                                                         | Description                                                                                                                                          | Parameters                                                                                                                                     | Test Defaults                                                                                                                                                      |
| ------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **MissingValueCount()**                                                                                                        | <ul><li>Column-level.</li><li>Counts the number and share of missing values.</li><li>Metric result: `count`, `share`.</li></ul>                      | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                | <ul><li>**No reference**: Fails if there are missing values.</li><li>**With reference**: Fails if share of missing values is >10% higher.</li></ul>                |
| **InRangeValueCount()**  <br /><br /> Example: <br /> `InRangeValueCount(`<br />`column="age",`<br />`left="1", right="18")`   | <ul><li>Column-level.</li><li>Counts the number and share of values in the set range.</li><li>Metric result: `count`, `share`.</li></ul>             | **Required**: <ul><li>`column`</li><li>`left`</li><li>`right`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: N/A.</li><li>**With reference**: Fails if column contains values out of the min-max reference range.</li></ul>                           |
| **OutRangeValueCount()**                                                                                                       | <ul><li>Column-level.</li><li>Counts the number and share of values out of the set range.</li><li>Metric result: `count`, `share`.</li></ul>         | **Required**: <ul><li>`column`</li><li>`left`</li><li>`right`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: N/A.</li><li>**With reference**: Fails if any value is out of min-max reference range.</li></ul>                                         |
| **InListValueCount()**                                                                                                         | <ul><li>Column-level.</li><li>Counts the number and share of values in the set list.</li><li>Metric result: `count`, `share`.</li></ul>              | **Required**: <ul><li>`column`</li><li>`values`</li></ul>**Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                | <ul><li>**No reference**: N/A.</li><li>**With reference**: Fails if any value is out of list.</li></ul>                                                            |
| **OutListValueCount()**  <br /><br /> Example: <br /> `OutListValueCount(`<br />`column="city",`<br />` values=["Lon", "NY"])` | <ul><li>Column-level.</li><li>Counts the number and share of values out of the set list.</li><li>Metric result: `count`, `share`.</li></ul>          | **Required**: <ul><li>`column`</li><li>`values`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>               | <ul><li>**No reference**: N/A.</li><li>**With reference**: Fails if any value is out of list.</li></ul>                                                            |
| **UniqueValueCount()**                                                                                                         | <ul><li>Column-level.</li><li>Counts the number and share of unique values.</li><li>Metric result: `values` (dict with `count, share`).</li></ul>    | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                | <ul><li>**No reference**: N/A.</li><li>**With reference**: Fails if the share of unique values differs by >10% (+/-).</li></ul>                                    |

## Dataset

Use for exploratory data analysis and data quality checks.

<Info>
  [Data definition](/docs/library/data_definition). You may need to map column types, ID and timestamp.
</Info>

### Dataset stats

Descriptive statistics.

| Metric                  | Description                                                                                                                                                                                                         | Parameters                                                             | Test Defaults                                                                                                  |
| ----------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------- |
| **DataSummaryPreset()** | <ul><li>Large Preset.</li><li>Combines `DatasetStats` and `ValueStats` for all or specified columns.</li><li>Metric result: for all Metrics.</li><li>[Preset page](/metrics/preset_data_summary)</li></ul>          | **Optional**: <ul><li>`columns`</li></ul>                              | As in individual Metrics.                                                                                      |
| **DatasetStats()**      | <ul><li>Small preset. </li><li> Dataset-level.</li><li>Calculates descriptive dataset stats, including columns by type, rows, missing values, empty columns, etc.</li><li>Metric result: for all Metrics.</li></ul> | None                                                                   | <ul><li>**No reference**: As in included Metrics</li><li>**With reference**: As in included Metrics.</li></ul> |
| **RowCount()**          | <ul><li> Dataset-level.</li><li>Counts the number of rows.</li><li>Metric result: `value`.</li></ul>                                                                                                                | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: N/A.</li><li>**With reference**: Fails if row count differs by >10%.</li></ul>       |
| **ColumnCount()**       | <ul><li> Dataset-level.</li><li>Counts the number of columns.</li><li>Metric result:  `value`.</li></ul>                                                                                                            | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: N/A.</li><li>**With reference**: Fails if not equal to reference.</li></ul>          |

### Dataset data quality

Dataset-level data quality metrics.

<Info>
  [Data definition](/docs/library/data_definition). You may need to map column types, ID and timestamp.
</Info>

| Metric                                          | Description                                                                                                                                                                            | Parameters                                                                                                      | Test Defaults                                                                                                                                                          |
| ----------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **ConstantColumnsCount()**                      | <ul><li> Dataset-level.</li><li>Counts the number of constant columns.</li><li>Metric result: `value`.</li></ul>                                                                       | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one constant column.</li><li>**With reference**: Fails if count is higher than in reference.</li></ul>            |
| **EmptyRowsCount()**                            | <ul><li> Dataset-level.</li><li>Counts the number of empty rows.</li><li>Metric result: `value`.</li></ul>                                                                             | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one empty row.</li><li>**With reference**: Fails if share differs by >10%.</li></ul>                              |
| **EmptyColumnsCount()**                         | <ul><li> Dataset-level.</li><li>Counts the number of empty columns.</li><li>Metric result: `value`.</li></ul>                                                                          | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one empty column.</li><li>**With reference**: Fails if count is higher than in reference.</li></ul>               |
| **DuplicatedRowCount()**                        | <ul><li> Dataset-level.</li><li>Counts the number of duplicated rows.</li><li>Metric result: `value`.</li></ul>                                                                        | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one duplicated row.</li><li>**With reference**: Fails if share differs by >10% (+/-).</li></ul>                   |
| **DuplicatedColumnsCount()**                    | <ul><li> Dataset-level.</li><li>Counts the number of duplicated columns.</li><li>Metric result: `value`.</li></ul>                                                                     | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one duplicated column.</li><li>**With reference**: Fails if count is higher than in reference.</li></ul>          |
| **DatasetMissingValueCount()**                  | <ul><li> Dataset-level.</li><li>Calculates the number and share of missing values.</li><li>Displays the number of missing values per column.</li><li>Metric result: `share`, `count`.</li></ul> | **Required**: <ul><li>`columns`</li></ul>**Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Fails if there are missing values.</li><li>**With reference**: Fails if share is >10% higher than reference (+/-).</li></ul>                 |
| **AlmostConstantColumnsCount()**                | <ul><li> Dataset-level.</li><li>Counts almost constant columns (95% identical values).</li><li>Metric result: `value`.</li></ul>                                                       | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one almost constant column.</li><li>**With reference**: Fails if count is higher than in reference.</li></ul>     |
| **ColumnsWithMissingValuesCount()**             | <ul><li> Dataset-level.</li><li>Counts columns with missing values.</li><li>Metric result: `value`.</li></ul>                                                                          | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one column with missing values.</li><li>**With reference**: Fails if count is higher than in reference.</li></ul> |

## Data Drift

Use to detect distribution drift for text and tabular data or over computed text descriptors. Checks 20+ drift methods listed separately: [text and tabular](/metrics/customize_data_drift).

<Info>
  [Data definition](/docs/library/data_definition). You may need to map column types, ID and timestamp.
</Info>

<Info>
  [Metrics explainers](/metrics/explainer_drift). Understand how data drift works.
</Info>

| Metric                                | Description                                                                                                                                                                                                                                                                                                                                | Parameters                                                                                                                                                                                                                                                                             | Test Defaults                                                                                                                            |
| ------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- |
| **DataDriftPreset()**                 | <ul><li>Large Preset. </li><li> Requires reference.</li><li>Calculates data drift for all or set columns. </li><li> Uses the default or set method.</li><li>Returns drift score for each column.</li><li>Visualizes all distributions.</li><li>Metric result: all Metrics.</li><li>[Preset page](/metrics/customize_data_drift).</li></ul> | **Optional**: <ul><li>`columns`</li><li>`method`</li><li>`cat_method`</li><li>`num_method`</li><li>`per_column_method`</li><li>`threshold`</li><li>`cat_threshold`</li><li>`num_threshold`</li><li>`per_column_threshold`</li></ul>See [drift options](/metrics/customize_data_drift). | <ul><li>**With reference**: Data drift defaults, depending on column type. See [drift methods](/metrics/customize_data_drift).</li></ul> |
| **DriftedColumnsCount()**             | <ul><li> Dataset-level. </li><li> Requires reference.</li><li>Calculates the number and share of drifted columns in the dataset.</li><li>Each column is tested for drift using the default algorithm or set method.</li><li>Returns only the total number of drifted columns.</li><li>Metric result: `count`, `share`.</li></ul>           | **Optional**: <ul><li>`columns`</li><li>`method`</li><li>`cat_method`</li><li>`num_method`</li><li>`per_column_method`</li><li>`threshold`</li><li>`cat_threshold`</li><li>`num_threshold`</li><li>`per_column_threshold`</li></ul>See [drift options](/metrics/customize_data_drift). | <ul><li>**With reference**: Fails if 50% of columns are drifted.</li></ul>                                                               |
| **ValueDrift()**                      | <ul><li>Column-level.</li><li> Requires reference.</li><li>Calculates data drift for a defined column (num, cat, text).</li><li>Visualizes distributions.</li><li>Metric result: `value`.</li></ul>                                                                                                                                        | **Required**: <ul><li>`column`</li></ul>**Optional:** <ul><li>`method`</li><li>`threshold`</li></ul>See [drift options](/metrics/customize_data_drift).                                                                                                                                | <ul><li>**With reference**: Data drift defaults, depending on column type. See [drift methods](/metrics/customize_data_drift).</li></ul> |


## Classification

Use to evaluate quality on a classification task (probabilistic, non-probabilistic, binary and multi-class).

<Info>
  [Data definition](/docs/library/data_definition). You may need to map prediction, target columns and classification type.
</Info>

### General

Use for binary classification and aggregated results for multi-class.

| Metric                         | Description                                                                                                                                                                 | Parameters                                                                                                                                                                                                                                                                       | Test Defaults                                                                                                                                                                        |
| ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **ClassificationPreset()**     | <ul><li>Large Preset with many classification Metrics and visuals.</li><li>See [Preset page](/metrics/preset_classification).</li><li>Metric result: all Metrics.</li></ul> | Optional: `probas_threshold`  .                                                                                                                                                                                                                                                  | As in individual Metrics.                                                                                                                                                            |
| **ClassificationQuality()**    | <ul><li>Small Preset.</li><li>Summarizes quality Metrics in a single widget.</li><li>Metric result: all Metrics.</li></ul>                                                  | Optional: `probas_threshold`                                                                                                                                                                                                                                                     | As in individual Metrics.                                                                                                                                                            |
| **Accuracy()**                 | <ul><li>Calculates accuracy.</li><li>Metric result: `value`.</li></ul>                                                                                                      | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                                                                                                                                           | <ul><li>**No reference**: Fails if lower than dummy model accuracy.</li><li>**With reference**: Fails if accuracy differs by >20%.</li></ul>                                         |
| **Precision()**                | <ul><li>Calculates precision.</li><li>Visualizations available: Confusion Matrix, PR Curve, PR Table.</li><li>Metric result: `value`.</li></ul>                             | **Required**: <ul><li>Set at least one visualization: `conf_matrix`, `pr_curve`, `pr_table`.</li></ul> **Optional**: <ul><li>`probas_threshold` (default: None or 0.5 for probabilistic classification)</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Fails if Precision is lower than the dummy model.</li><li>**With reference**: Fails if Precision differs by >20%.</li></ul>                                |
| **Recall()**                   | <ul><li>Calculates recall.</li><li>Visualizations available: Confusion Matrix, PR Curve, PR Table.</li><li>Metric result: `value`.</li></ul>                                | **Required**: <ul><li>Set at least one visualization: `conf_matrix`, `pr_curve`, `pr_table`.</li></ul> **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                         | <ul><li>**No reference**: Fails if lower than dummy model recall.</li><li>**With reference**: Fails if Recall differs by >20%.</li></ul>                                             |
| **F1Score()**                  | <ul><li>Calculates F1 Score.</li><li>Metric result: `value`.</li></ul>                                                                                                      | **Required**: <ul><li>Set at least one visualization: `conf_matrix`.</li></ul> **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                 | <ul><li>**No reference**: Fails if lower than dummy model F1.</li><li>**With reference**: Fails if F1 differs by >20%.</li></ul>                                                     |
| **TPR()**                      | <ul><li>Calculates True Positive Rate (TPR).</li><li>Metric result: `value`.</li></ul>                                                                                      | **Required**: <ul><li>Set at least one visualization: `pr_table`.</li></ul> **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                    | <ul><li>**No reference**: Fails if TPR is lower than the dummy model.</li><li>**With reference**: Fails if TPR differs by >20%.</li></ul>                                            |
| **TNR()**                      | <ul><li>Calculates True Negative Rate (TNR).</li><li>Metric result: `value`.</li></ul>                                                                                      | **Required**: <ul><li>Set at least one visualization: `pr_table`.</li></ul> **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                    | <ul><li>**No reference**: Fails if TNR is lower than the dummy model.</li><li>**With reference**: Fails if TNR differs by >20%.</li></ul>                                            |
| **FPR()**                      | <ul><li>Calculates False Positive Rate (FPR).</li><li>Metric result: `value`.</li></ul>                                                                                     | **Required**: <ul><li>Set at least one visualization: `pr_table`.</li></ul> **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                    | <ul><li>**No reference**: Fails if FPR is higher than the dummy model.</li><li>**With reference**: Fails if FPR differs by >20%.</li></ul>                                           |
| **FNR()**                      | <ul><li>Calculates False Negative Rate (FNR).</li><li>Metric result: `value`.</li></ul>                                                                                     | **Required**: <ul><li>Set at least one visualization: `pr_table`.</li></ul> **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                    | <ul><li>**No reference**: Fails if FNR is higher than the dummy model.</li><li>**With reference**: Fails if FNR differs by >20%.</li></ul>                                           |
| **LogLoss()**                  | <ul><li>Calculates Log Loss.</li><li>Metric result: `value`.</li></ul>                                                                                                      | **Required**: <ul><li>Set at least one visualization: `pr_table`.</li></ul> **Optional**: <ul><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                                               | <ul><li>**No reference**: Fails if LogLoss is higher than the dummy model (equals 0.5 for a constant model).</li><li>**With reference**: Fails if LogLoss differs by >20%.</li></ul> |
| **RocAUC()**                   | <ul><li>Calculates ROC AUC.</li><li>Can visualize PR curve or table.</li><li>Metric result: `value`.</li></ul>                                                              | **Required**: <ul><li>Set at least one visualization: `pr_table`, `roc_curve`.</li></ul> **Optional**: <ul><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                                  | <ul><li>**No reference**: Fails if ROC AUC is ≤ 0.5.</li><li>**With reference**: Fails if ROC AUC differs by >20%.</li></ul>                                                         |

Dummy metrics:

<Accordion title="Dummy model quality" defaultOpen={false}>
  Use these Metics to get the quality of a dummy model created on the same data (based on heuristics). You can compare your model quality to verify that it's better than random. These Metrics serve as a baseline in automated testing.

  | Metric                           | Description                                                                                             | Parameters | Test Defaults |
  | -------------------------------- | ------------------------------------------------------------------------------------------------------- | ---------- | ------------- |
  | **ClassificationDummyQuality()** | <ul><li>Small Preset summarizing quality of a dummy model.</li><li>Metric result: all Metrics</li></ul> | N/A        | N/A           |
  | **DummyPrecision()**             | <ul><li>Calculates precision for a dummy model.</li><li>Metric result: `value`.</li></ul>               | N/A        | N/A           |
  | **DummyRecall()**                | <ul><li>Calculates recall for a dummy model.</li><li>Metric result: `value`.</li></ul>                  | N/A        | N/A           |
  | **DummyF1()**                    | <ul><li>Calculates F1 Score for a dummy model.</li><li>Metric result: `value`.</li></ul>                | N/A        | N/A           |
</Accordion>

### By label

Use when you have multiple classes and want to evaluate quality separately.

| Metric                             | Description                                                                                                                  | Parameters                                                                                                        | Test Defaults                                                                                                                                         |   |
| ---------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------- | - |
| **ClassificationQualityByLabel()** | <ul><li>Small Preset summarizing classification quality Metrics by label.</li><li>Metric result: all Metrics.</li></ul>      | None                                                                                                              | As in individual Metrics.                                                                                                                             |   |
| **PrecisionByLabel()**             | <ul><li>Calculates precision by label in multiclass classification.</li><li>Metric result (dict): `label: value`. </li></ul> | **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Fails if Precision is lower than the dummy model.</li><li>**With reference**: Fails if Precision differs by >20%.</li></ul> |   |
| **F1ByLabel()**                    | <ul><li>Calculates F1 Score by label in multiclass classification.</li><li>>Metric result (dict): `label: value`.</li></ul>  | **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Fails if F1 is lower than the dummy model.</li><li>**With reference**: Fails if F1 differs by >20%.</li></ul>               |   |
| **RecallByLabel()**                | <ul><li>Calculates recall by label in multiclass classification.</li><li>>Metric result (dict): `label: value`</li></ul>     | **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Fails if Recall is lower than the dummy model.</li><li>**With reference**: Fails if Recall differs by >20%.</li></ul>       |   |
| **RocAUCByLabel()**                | <ul><li>Calculates ROC AUC by label in multiclass classification.</li><li>Metric result (dict): `label: value`</li></ul>     | **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Fails if ROC AUC is ≤ 0.5.</li><li>**With reference**: Fails if ROC AUC differs by >20%.</li></ul>                          |   |

## Regression

Use to evaluate the quality of a regression model.

<Info>
  [Data definition](/docs/library/data_definition). You may need to map prediction and target columns.
</Info>

| Metric                | Description                                                                                                                                                                                                    | Parameters                                                                                                                                                                             | Test Defaults                                                                                                                                                                                   |
| --------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **RegressionPreset**  | <ul><li>Large Preset. </li><li> Includes a wide range of regression metrics with rich visuals.</li><li>Metric result: all metrics.</li><li>See [Preset page](/metrics/preset_regression).</li></ul>            | None.                                                                                                                                                                                  | As in individual metrics.                                                                                                                                                                       |
| **RegressionQuality** | <ul><li>Small Preset. </li><li> Summarizes key regression metrics in a single widget.</li><li>Metric result: all metrics.</li></ul>                                                                            | None.                                                                                                                                                                                  | As in individual metrics.                                                                                                                                                                       |
| **MeanError()**       | <ul><li>Calculates the mean error.</li><li>Visualizations available: Error Plot, Error Distribution, Error Normality.</li><li>Metric result: `mean_error`, `error_std`.</li></ul>                              | **Required**: <ul><li>Set at least one visualization: `error_plot`, `error_distr`, `error_normality`.</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests). Use  `mean_tests` and `std_tests`. </li></ul> | <ul><li>**No reference/With reference**: Expect ME to be near zero. Fails if Mean Error is skewed and condition is violated: `eq = approx(absolute=0.1 * error_std)`.</li></ul>                 |
| **MAE()**             | <ul><li>Calculates Mean Absolute Error (MAE).</li><li>Visualizations available: Error Plot, Error Distribution, Error Normality.</li><li>Metric result: `mean_absolute_error`, `absolute_error_std`.</li></ul> | **Required**: <ul><li>Set at least one visualization: `error_plot`, `error_distr`, `error_normality`.</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests). Use  `mean_tests` and `std_tests`. </li></ul> | <ul><li>**No reference**: Fails if MAE is higher than the dummy model predicting the median target value.</li><li>**With reference**: Fails if MAE differs by >10%.</li></ul>                   |
| **RMSE()**            | <ul><li>Calculates Root Mean Square Error (RMSE).</li><li>Metric result: `rmse`.</li></ul>                                                                                                                     | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                                                 | <ul><li>**No reference**: Fails if RMSE is higher than the dummy model predicting the mean target value.</li><li>**With reference**: Fails if RMSE differs by >10%.</li></ul>                   |
| **MAPE()**            | <ul><li>Calculates Mean Absolute Percentage Error (MAPE).</li><li>Visualizations available: Percentage Error Plot.</li><li>Metric result: `mean_perc_absolute_error`, `perc_absolute_error_std`.</li></ul>     | **Required**: <ul><li>Set at least one visualization: `perc_error_plot`.</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                              | <ul><li>**No reference**: Fails if MAPE is higher than the dummy model predicting the weighted median target value.</li><li>**With reference**: Fails if MAPE differs by >10%.</li></ul>        |
| **R2Score()**         | <ul><li>Calculates R² (Coefficient of Determination).</li><li>Metric result: `r2score`.</li></ul>                                                                                                              | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                                                 | <ul><li>**No reference**: Fails if R² ≤ 0.</li><li>**With reference**: Fails if R² differs by >10%.</li></ul>                                                                                   |
| **AbsMaxError()**     | <ul><li>Calculates Absolute Maximum Error.</li><li>Metric result: `abs_max_error`.</li></ul>                                                                                                                   | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                                                 | <ul><li>**No reference**: Fails if absolute maximum error is higher than the dummy model predicting the median target value.</li><li>**With reference**: Fails if it differs by >10%.</li></ul> |

Dummy metrics:

<Accordion title="Dummy model quality" defaultOpen={false}>
  Use these Metics to get the baseline quality for regression: they use optimal constants (varies by the Metric). These Metrics serve as a baseline in automated testing.

  | Metric                       | Description                                                                                                                                                         | Parameters | Test Defaults |
  | ---------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------- | ------------- |
  | **RegressionDummyQuality()** | <ul><li>Small Preset summarizing quality of a dummy model.</li><li>Metric result: all Metrics</li></ul>                                                             | N/A        | N/A           |
  | **DummyMeanError()**         | <ul><li>Calculates Mean Error for a dummy model.</li><li>Metric result: `mean_error`, `error std`.</li></ul>                                                        | N/A        | N/A           |
  | **DummyMAE()**               | <ul><li>Calculates Mean Absolute Error (MAE) for a dummy model.</li><li>Metric result: `mean_absolute_error`, `absolute_error_std`.</li></ul>                       | N/A        | N/A           |
  | **DummyMAPE()**              | <ul><li>Calculates Mean Absolute Percentage Error (MAPE) for a dummy model.</li><li>Metric result: `mean_perc_absolute_error`, `perc_absolute_error std`.</li></ul> | N/A        | N/A           |
  | **DummyRMSE()**              | <ul><li>Calculates Root Mean Square Error (RMSE) for a dummy model.</li><li>Metric result: `rmse`.</li></ul>                                                        | N/A        | N/A           |
  | **DummyR2()**                | <ul><li>Calculates Calculates R² (Coefficient of Determination) for a dummy model.</li><li>Metric result: `r2score`.</li></ul>                                      | N/A        | N/A           |
</Accordion>

## Ranking

Use to evaluate ranking, search / retrieval or recommendations.

<Info>
  [Data definition](/docs/library/data_definition). You may need to map prediction and target columns and ranking type.
</Info>

<Info>
  [**Metric explainers**](/metrics/explainer_recsys)**.** Check ranking metrics explainers.
</Info>

| Metric                              | Description                                                                                                                                                                         | Parameters                                                                                                                                                     | Test Defaults                                                                                                                           |
| ----------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |
| **RecallTopK()**                    | <ul><li>Calculates Recall at the top K retrieved items.</li><li>Metric result: `value`.</li></ul>                                                                                   | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`no_feedback_users`</li><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Tests if recall > 0.</li><li>**With reference**: Fails if Recall differs by >10%.</li></ul>                   |
| **FBetaTopK()**                     | <ul><li>Calculates F-beta score at the top K retrieved items.</li><li>Metric result: `value`.</li></ul>                                                                             | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`no_feedback_users`</li><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Tests if F-beta > 0.</li><li>**With reference**: Fails if F-beta differs by >10%.</li></ul>                   |
| **PrecisionTopK()**                 | <ul><li>Calculates Precision at the top K retrieved items.</li><li>Metric result: `value`.</li></ul>                                                                                | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`no_feedback_users`</li><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Tests if Precision > 0.</li><li>**With reference**: Fails if Precision differs by >10%.</li></ul>             |
| **MAP()**                           | <ul><li>Calculates Mean Average Precision at the top K retrieved items.</li><li>Metric result: `value`.</li></ul>                                                                   | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`no_feedback_users`</li><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Tests if MAP > 0.</li><li>**With reference**: Fails if MAP differs by >10%.</li></ul>                         |
| **NDCG()**                          | <ul><li>Calculates Normalized Discounted Cumulative Gain at the top K retrieved items.</li><li>Metric result: `value`.</li></ul>                                                    | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`no_feedback_users`</li><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Tests if NDCG > 0.</li><li>**With reference**: Fails if NDCG differs by >10%.</li></ul>                       |
| **MRR()**                           | <ul><li>Calculates Mean Reciprocal Rank at the top K retrieved items.</li><li>Metric result: `value`.</li></ul>                                                                     | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`no_feedback_users`</li><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Tests if MRR > 0.</li><li>**With reference**: Fails if MRR differs by >10%.</li></ul>                         |
| **HitRate()**                       | <ul><li>Calculates Hit Rate at the top K retrieved items.</li><li>Metric result: `value`.</li></ul>                                                                                 | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`no_feedback_users`</li><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Tests if Hit Rate > 0.</li><li>**With reference**: Fails if Hit Rate differs by >10%.</li></ul>               |
| **ScoreDistribution()**             | <ul><li>Computes the predicted score entropy (KL divergence). </li><li> Applies only when the recommendations\_type is a score..</li><li>Metric result: `value`.</li></ul>          | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                     | <ul><li>**No reference**:`value`</li><li>**With reference**: `value`.</li></ul>                                                         |
