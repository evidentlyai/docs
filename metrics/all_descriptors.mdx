---
title: "All Descriptors"
description: "Reference page for all row-level text and LLM evals."
---

<Info>
  For an intro, read about [Core Concepts](/docs/library/overview) and check the [LLM Quickstart](/quickstart_llm).
</Info>

## Deterministic evals

Programmatic and heuristics-based evaluations.

### Pattern match

Check for general pattern matching.

| Name             | Description                                                                                                                                                                                   | Parameters                                                                                                                  |
| ---------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- |
| **ExactMatch()** | <ul><li>Checks if the column contents matches between two provided columns.</li><li>Returns True/False for every input.</li><li>Example: `ExactMatch(columns=["answer", "target"])`</li></ul> | **Required:** <ul><li>`columns`</li></ul>**Optional:** <ul><li>`alias` </li></ul>                                           |
| **RegExp()**     | <ul><li>Matches the text against a set regular expression.</li><li>Returns True/False for every input.</li><li>Example: `RegExp(reg_exp=r"^I")`</li></ul>                                     | **Required:** <ul><li>`reg_exp`</li></ul>**Optional:** <ul><li> `alias`</li></ul>                                           |
| **BeginsWith()** | <ul><li>Checks if the text starts with a given combination. </li><li>Returns True/False for every input.</li><li>Example: `BeginsWith(prefix="How")`</li></ul>                                | **Required:** <ul><li>`prefix`</li></ul>**Optional:** <ul><li>`alias`</li><li> `case_sensitive = True` or `False`</li></ul> |
| **EndsWith()**   | <ul><li>Checks if the text ends with a given combination.</li><li>Returns True/False for every input.</li><li>Example: `EndsWith(suffix="Thank you."`)</li></ul>                              | **Required:** <ul><li>`suffix`</li></ul>**Optional:** <ul><li>`alias`</li><li>`case_sensitive = True` or `False`</li></ul>  |

### Content checks

Verify presence of specific words, items or components.

| Name                 | Description                                                                                                                                                                                                                                                                                                      | Parameters                                                                                                                                                             |
| -------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Contains()**       | <ul><li>Checks if the text contains **any** or **all** specified items (e.g., competitor names).</li><li>Returns True/False for every input.</li><li>Example: `Contains(items=["chatgpt"])`</li></ul>                                                                                                            | **Required:** <ul><li>`items: List[str]`</li></ul>**Optional:** <ul><li>`alias`</li><li> `mode = any` or `all` </li><li> `case_sensitive = True` or `False`</li></ul>  |
| **DoesNotContain()** | <ul><li>Checks if the text does not contain the specified items (e.g., forbidden expressions).</li><li>Returns True/False for every input.</li><li>Example: `DoesNotContain(items=["as a large language model"])`</li></ul>                                                                                      | **Required:** <ul><li>`items: List[str]`</li></ul>**Optional:** <ul><li>`alias` </li><li>`mode = all` </li><li> `case_sensitive = True` or `False`</li></ul>           |
| **IncludesWords()**  | <ul><li>Checks if the text includes **any** or **all** specified words. </li><li>Considers only vocabulary words.</li><li>Returns True/False for every input.</li><li>Example: `IncludesWords(words_list=['booking', 'hotel', 'flight'])`</li></ul>                                                              | **Required:** <ul><li>`words_list: List[str]`</li></ul>**Optional:** <ul><li>`alias` </li><li> `mode = any` or `all` </li><li> `lemmatize = True` or `False`</li></ul> |
| **ExcludesWords()**  | <ul><li>Checks if the texts excludes all specified words (e.g. profanity lists). </li><li>Considers only vocabulary words.</li><li>Returns True/False for every input.</li><li>Example: `ExcludesWords(words_list=['buy', 'sell', 'bet'])`</li></ul>                                                             | **Required:** <ul><li>`words_list: List[str]`</li></ul>**Optional:** <ul><li>`alias` </li><li> `mode = all` </li><li> `lemmatize = True` or `False`</li></ul>          |
| **ItemMatch()**      | <ul><li>Checks if the text contains **any** or **all** specified items.</li><li> The item list is specific to each row and provided in a separate column.</li><li>Returns True/False for each row.</li><li>Example: `ItemMatch(["Answer", "Expected_items"])`</li></ul>                                          | **Required:** <ul><li>`columns`</li></ul>**Optional:** <ul><li> `alias` </li><li> `mode = all` or `any` </li><li> `case_sensitive = True` or `False`</li></ul>         |
| **ItemNoMatch()**    | <ul><li>Checks if the text excludes **all** specified items.</li><li> The item list is specific to each row and provided in a separate column.</li><li>Returns True/False for each row.</li><li>Example: `ItemMatch(["Answer", "Forbidden_items"])`</li></ul>                                                    | **Required:** <ul><li>`columns`</li></ul>**Optional:** <ul><li>`alias` </li><li> `mode = all` </li><li>`case_sensitive = True` or `False`</li></ul>                    |
| **WordMatch()**      | <ul><li>Checks if the text includes **any** or **all** specified words. </li><li> Word list is specific to each row and provided in a separate column. </li><li>Considers only vocabulary words.</li><li>Returns True/False for every input.</li><li>Example: `WordMatch(["Answer", "Expected_words"]`</li></ul> | **Required:**<ul><li> `columns`</li></ul>**Optional:** <ul><li>`alias` </li><li> `mode = any` or `all` </li><li> `lemmatize = True` or `False`</li></ul>               |
| **WordNoMatch()**    | <ul><li>Checks if the text excludes **all** specified words. </li><li> Word list is specific to each row and provided in a separate column. </li><li>Considers only vocabulary words.</li><li>Returns True/False for every input.</li><li>Example: `WordNoMatch(["Answer", "Forbidden_words"]`</li></ul>         | **Required:** <ul><li>`columns`str</li></ul>**Optional:** <ul><li>`alias` </li><li> `mode = all` </li><li> `lemmatize = True` or `False`</li></ul>                     |
| **ContainsLink()**   | <ul><li>Checks if the column contains at least one valid URL.</li><li>Returns True/False for each row.</li></ul>                                                                                                                                                                                                 | **Optional:** <ul><li>`alias`</li></ul>                                                                                                                                |

### Syntax validation

Validate structured data formats or code syntax.

| Name                  | Description                                                                                                                                                                                                                                                                                                                                                                                                                         | Parameters                                                                                                                                                                       |
| --------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **IsValidJSON()**     | <ul><li>Checks if the column contains a valid JSON.</li><li>Returns True/False for every input.</li></ul>                                                                                                                                                                                                                                                                                                                           | **Optional:** <ul><li>`alias`</li></ul>                                                                                                                                          |
| **JSONSchemaMatch()** | <ul><li>Checks if the column contains a valid JSON object matching the expected **schema**: all keys are present and values are not `None`. </li><li>Exact match mode checks no extra keys are present.</li><li> Optional type validation for each key. </li><li>Returns True/False for each input.</li><li>Example: `JSONSchemaMatch(expected_schema={"name": str, "age": int}, exact_match=False, validate_types=True)`</li></ul> | **Required:** <ul><li>`expected_schema: Dict[str, type]`</li></ul> **Optional:** <ul><li> `exact_match = True` or `False` </li><li> `validate_types = True` or `False`</li></ul> |
| **JSONMatch()**       | <ul><li>Checks if the column contains a valid JSON object matching a JSON provided in a reference column. </li><li>Matches **key-value pairs** irrespective of order.</li><li>Whitespace outside of the actual values (e.g., spaces or newlines) is ignored.</li><li>Returns True/False for every input.</li><li>Example: `JSONMatch(first_column="Json1", second_column="Json2"),`</li></ul>                                       | **Required:** <ul><li>`first_column`</li><li>`second_column`</li></ul>**Optional:** <ul><li>`alias`</li></ul>                                                                    |
| **IsValidPython()**   | <ul><li>Checks if the column contains valid Python code without syntax errors.</li><li>Returns True/False for every input.</li></ul>                                                                                                                                                                                                                                                                                                | **Optional:** <ul><li>`alias`</li></ul>                                                                                                                                          |
| **IsValidSQL()**      | <ul><li>Checks if the column contains a valid SQL query without executing the query.</li><li>Returns True/False for every input.</li></ul>                                                                                                                                                                                                                                                                                          | **Optional:** <ul><li>`alias`</li></ul>                                                                                                                                          |

### Text stats

Descriptive text statistics.

| Name                               | Descriptor                                                                                                                                              | Parameters                                                                 |
| :--------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------- |
| **TextLength()**                   | <ul><li>Measures the length of the text in symbols.</li><li>Returns an absolute number.</li></ul>                                                       | **Optional:** <ul><li>`alias`</li></ul>                                    |
| **OOVWordsPercentage()**           | <ul><li>Calculates the percentage of out-of-vocabulary words based on imported NLTK vocabulary.</li><li>Returns a score on a scale: 0 to 100.</li></ul> | **Optional:** <ul><li>`alias`</li><li>`ignore_words: Tuple = ()`</li></ul> |
| **NonLetterCharacterPercentage()** | <ul><li>Calculates the percentage of non-letter characters.</li><li>Returns a score on a scale: 0 to 100.</li></ul>                                     | **Optional:** <ul><li>`alias`</li></ul>                                    |
| **SentenceCount()**                | <ul><li>Counts the number of sentences in the text.</li><li>Returns an absolute number.</li></ul>                                                       | **Optional:** <ul><li>`alias`</li></ul>                                    |
| **WordCount()**                    | <ul><li>Counts the number of words in the text.</li><li>Returns an absolute number.</li></ul>                                                           | **Optional:** <ul><li>`alias`</li></ul>                                    |

### Custom

Implement your own programmatic checks.

| Name                          | Descriptor                                                                                                                                                                                                                                                             | Parameters                                                                                                                            |
| :---------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- |
| **CustomDescriptor()**        | <ul><li>Implements a custom check for specific column(s) as a Python function.</li><li>Use it to run your own programmatic checks. </li><li>Returns score and/or label as specified.</li><li>Can accept and return multiple columns. </li></ul>                        | **Optional:** <ul><li>`alias`</li><li>`func: callable` </li></ul>See [how to add a custom descriptor](/metrics/customize_descriptor). |
| **CustomColumnsDescriptor()** | <ul><li>Implements a custom check as a Python function that can be applied to any column in the dataset.</li><li>Use it to run your own programmatic checks. </li><li>Returns score and/or label as specified.</li><li>Accepts and returns a single column. </li></ul> | **Optional:** <ul><li>`alias`</li><li>`func: callable` </li></ul>See [how to add a custom descriptor](/metrics/customize_descriptor). |

## LLM-based evals

Using an external LLMs with an evaluation prompt. You can specify the LLM to use as an evaluator.

### Custom

LLM judge templates.

| Name          | Descriptor                                                                                                                                                                                                   | Parameters                                                                                                                                                                                                |
| :------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **LLMEval()** | <ul><li>Scores the text using user-defined criteria.</li><li>You must specify provider, model and use prompt template to formulate the criteria. </li><li>Returns score and/or label as specified.</li></ul> | **Optional:** <ul><li>`alias`</li><li>`template`</li><li>`provider`</li><li>`model`</li><li>`additional_columns: dict`</li><li>See [custom LLM judge parameters](/metrics/customize_llm_judge).</li></ul> |

### RAG

RAG-specific evals for retrieval and generation. ([Tutorial](/examples/LLM_rag_evals)).

| Name                        | Descriptor                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Parameters                                                                                                                                                                                                                                                                                                                                          |
| :-------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **ContextQualityLLMEval()** | <ul><li>Evaluates if the context provides sufficient information to answer the question.</li><li>Returns a label (VALID or INVALID) or a score.</li><li>Run over the "context" column and pass the `question` column as a parameter.</li><li>Example: `ContextQualityLLMEval("Context", question="Question")`</li></ul>                                                                                                                                                           |  **Required:** <ul><li>`question`</li></ul> **Optional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul>                                                                                                                                                                                     |
| **ContextRelevance()**      | <ul><li>Checks if the context is relevant to the given question (0 to 1) for multiple context chunks.</li><li>Pass all context chunks as a list in the `context` column.</li><li>Uses semantic similarity (default) or LLM. </li><li>Aggregates relevance: `mean` (default) or `hit` (at least one chunk is relevant). </li><li>Example: `ContextRelevance("Question", "Context", output_scores=True, aggregation_method="hit", method="llm")`</li></ul> | **Required:** <ul><li>`input`</li><li>`contexts`</li></ul> **Optional:** <ul><li>`output_scores`: `False` or `True` </li><li>`method`: `semantic_similarity` or `llm`</li><li>`aggregation_method`: `mean` or `hit` </li><li>`aggregation_method_params={"threshold":0.95}` (set the relevance threshold as greater or equal, 0.8 by default)</li><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul> |
| **FaithfulnessLLMEval()**   | <ul><li>Assesses whether the response stays faithful to the given context.Checks for hallucinations or unsupported claims.</li><li>Returns a label (FAITHFUL or UNFAITHFUL) or a score.</li><li>Run over the "response" column and pass the `context` column as a parameter.</li><li>Example: `FaithfulnessLLMEval("Response", context="Context")`</li></ul>                                                                                                             | **Required:** <ul><li>`context`</li></ul> **Optional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul>                                                                                                                                                              |
| **CompletenessLLMEval()**   | <ul><li>Determines whether the response fully uses the information provided in the context.</li><li>Returns a label (COMPLETE or INCOMPLETE) or a score.</li><li>Run over the "response" column and pass the `context` column as a parameter.</li><li>Example: `CompletenessLLMEval("Response", context="Context")`</li></ul>                                                                                                                                                     | **Required:** <ul><li>`context`</li></ul> **Optional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul>                                                                                                                                                              |

### Generation

Evals for varied generation scenarios.

| Name                     | Descriptor                                                                                                                                                                                                                                                                                                                                                                                                          | Parameters                                                                                                                                                                                   |
| :----------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **CorrectnessLLMEval()** | <ul><li>Evaluates the correctness of a response by comparing it with the target output.</li><li>Useful for RAG or any LLM generation where you have a ground truth output</li><li>Returns a label (CORRECT or INCORRECT) or a score.</li><li>Run over the "response" column and pass the `target_output` column as a parameter.</li><li>Example: `CorrectnessLLMEval("Response", target_output="Target")`</li></ul> | **Required:** <ul><li>`target_output`</li></ul> **Optional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul> |
| **DeclineLLMEval()**     | <ul><li>Detects if the text contains a refusal or rejection. </li><li>Useful to detect instances where an LLM denies the user response.</li><li>Returns a label (DECLINE or OK) or a score.</li></ul>                                                                                                                                                                                                               | **Optional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul>                                                 |
| **PIILLMEval()**         | <ul><li>Detects texts containing PII (Personally Identifiable Information).</li><li>Returns a label (PII or OK) or a score.</li></ul>                                                                                                                                                                                                                                                                               | **Optional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul>                                                 |
| **NegativityLLMEval()**  | <ul><li>Detects negative texts.</li><li>Returns a label (NEGATIVE or POSITIVE) or a score.</li></ul>                                                                                                                                                                                                                                                                                                                | **Optional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul>                                                 |
| **BiasLLMEval()**        | <ul><li>Detects biased texts.</li><li>Returns a label (BIAS or OK) or a score.</li></ul>                                                                                                                                                                                                                                                                                                                            | **Optional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul>                                                 |
| **ToxicityLLMEval()**    | <ul><li>Detects toxic texts.</li><li>Returns a label (TOXICITY or OK) or a score.</li></ul>                                                                                                                                                                                                                                                                                                                         | **Optional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul>                                                 |

## ML-based evals

Use pre-trained machine learning or embedding models.

| Name                      | Descriptor                                                                                                                                                                                                                                                                                                                                                                             | Parameters                                                                                                                           |
| :------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |
| **SemanticSimilarity()**  | <ul><li>Calculates pairwise semantic similarity (Cosine Similarity) between two columns using a sentence embeddings model [`all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2).</li><li>Returns a score from 0 to 1: (0: different, 0.5: unrelated, 1: identical) </li><li>Example use: `SemanticSimilarity(columns=["Question", "Answer"])`.</li></ul> | **Required:** <ul><li>`columns`</li></ul>**Optional:** <ul><li> `alias`</li></ul>                                                    |
| **BERTScore()**           | <ul><li>Calculates similarity between two text columns based on token embeddings.</li><li>Returns [BERTScore](https://arxiv.org/pdf/1904.09675) (F1 Score).</li><li>Example use: `BERTScore(columns=["Answer", "Target"])`.</li></ul>                                                                                                                                                  | **Required:** <ul><li>`columns`</li></ul>**Optional:** <ul><li>`model`</li><li>`tfidf_weighted`</li><li>`alias`</li></ul>            |
| **Sentiment()**           | <ul><li>Analyzes text sentiment using a word-based model from NLTK.</li><li>Returns a score: -1 (negative) to 1 (positive).</li></ul>                                                                                                                                                                                                                                                            | **Optional:** <ul><li>`alias`</li></ul>                                                                                              |
| **HuggingFace()**         | <ul><li>Scores the text using a user-selected HuggingFace model.</li><li>See [HuggingFace descriptor docs](/metrics/customize_hf_descriptor) for example models.</li></ul>                                                                                                                                                                                                             | **Optional:** <ul><li>`alias`</li><li>See [docs](/metrics/customize_hf_descriptor).</li></ul>                                        |
| **HuggingFaceToxicity()** | <ul><li>Detects hate speech using a [`roberta-hate-speech`](https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target) model.</li><li>Returns predicted probability for the “hate” label. Scale: 0 to 1.</li></ul>                                                                                                                                                       | **Optional:** <ul><li>`toxic_label`(default: `hate`)</li><li>`alias`</li><li>See [docs](/metrics/customize_hf_descriptor).</li></ul> |