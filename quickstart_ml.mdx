---
title: 'Data and ML checks'
description: 'Run a simple evaluation for tabular data'
---

import CloudSignup from '/snippets/cloud_signup.mdx';
import CreateProject from '/snippets/create_project.mdx';

<Info>
  Need help? Ask on [Discord](https://discord.com/invite/xZjKRaNp8b).
</Info>

Evidently helps you run tests and evaluations for your production ML systems. This includes:
- evaluating prediction quality (e.g. classification or regression accuracy)
- input data quality (e.g. missing values, out-of-range features)
- data and prediction drift.

Evaluating distribution shifts ([data drift](https://www.evidentlyai.com/ml-in-production/data-drift)) in ML inputs and predictions is a typical use case that helps you detect shifts in the model quality and environment even without ground truth labels. 

In this Quickstart, you'll run a simple data drift report in Python and view the results in Evidently Cloud. If you want to stay fully local, you can also do that - just skip a couple steps.

## 1. Set up your environment

For a fully local flow, skip steps 1.1 and 1.3.

### 1.1. Set up Evidently Cloud

<CloudSignup />

### 1.2. Installation and imports

Install the Evidently Python library:

```python
!pip install evidently
```

Components to run the evals:

```python
import pandas as pd
from sklearn import datasets
    
from evidently import Dataset
from evidently import DataDefinition
from evidently import Report
from evidently.presets import DataDriftPreset, DataSummaryPreset 
```

Components to connect with Evidently Cloud:

```python
from evidently.ui.workspace import CloudWorkspace
```

### 1.3. Create a Project

<CreateProject />


## 2. Prepare a toy dataset

Let's import a toy dataset with tabular data:

```python
adult_data = datasets.fetch_openml(name="adult", version=2, as_frame="auto")
adult = adult_data.frame
```

<Accordion title="Have trouble downloading the data?" defaultOpen={false}>
  If OpenML is not available, you can download the same dataset from here:

  ```python
  url = "https://github.com/evidentlyai/evidently/blob/main/test_data/adults.parquet?raw=true"
  adult = pd.read_parquet(url, engine='pyarrow')
  ```
</Accordion>

Let's split the data into two and introduce some artificial drift for demo purposes. `Prod` data will include people with education levels unseen in the reference dataset:

```python
adult_ref = adult[~adult.education.isin(["Some-college", "HS-grad", "Bachelors"])]
adult_prod = adult[adult.education.isin(["Some-college", "HS-grad", "Bachelors"])]
```

Map the column types:

```python
schema = DataDefinition(
    numerical_columns=["education-num", "age", "capital-gain", "hours-per-week", "capital-loss", "fnlwgt"],
    categorical_columns=["education", "occupation", "native-country", "workclass", "marital-status", "relationship", "race", "sex", "class"],
    )
```

Create Evidently Datasets to work with:

```python
eval_data_1 = Dataset.from_pandas(
    pd.DataFrame(adult_prod),
    data_definition=schema
)
```

```python
eval_data_2 = Dataset.from_pandas(
    pd.DataFrame(adult_ref),
    data_definition=schema
)
```

`Eval_data_2` will be our reference dataset we'll compare against. 

## 3. Get a Report

Let's generate a Data Drift preset that will check for statistical distribution changes between all columns in the dataset. 

```python
report = Report([
    DataDriftPreset() 
])

my_eval = report.run(eval_data_1, eval_data_2)
```

<Info>
You can [customize drift parameters](/metrics/customize_data_drift) by choosing different methods and thresholds. In our case we proceed as is so [default tests](/metrics/explainer_drift) selected by Evidently will apply. 
</Info>

## 4. Explore the results

**Local preview**. In a Python environment like Jupyter notebook or Colab, run:

```python
my_eval
```

This will render the Report directly in the notebook cell. You can also get a JSON or Python dictionary, or save as an external HTML file.

```python
# my_eval.json()
# my_eval.dict()
# my_report.save_html(“file.html”)
```

Local Reports are great for one-off evaluations. To run continuous monitoring (e.g. track the share of drifting features over time), keep track of the results and collaborate with others, upload the results to Evidently Platform.


**Upload the Report** with summary results:

```python
ws.add_run(project.id, my_eval, include_data=False)
```

**View the Report**. Go to [Evidently Cloud](https://app.evidently.cloud/), open your Project, navigate to "Reports" in the left and open the Report. You will see the summary with scores and Test results.

![](/images/examples/data_drift_quickstart.png)


## 5. Get a Dashboard (Optional)

As you run repeated evals, you may want to track the results in time by creating a Dashboard. Evidently lets you configure the dashboard in the UI or using dashboards-as-code. 

```python
from evidently.sdk.models import PanelMetric
from evidently.sdk.panels import DashboardPanelPlot

project.dashboard.add_panel(
             DashboardPanelPlot(
                title="Dataset column drift",
                subtitle = "Share of drifted columns",
                size="half",
                values=[
                    PanelMetric(
                        legend="Share",
                        metric="DriftedColumnsCount",
                        metric_labels={"value_type": "share"} 
                    ),
                ],
                plot_params={"plot_type": "line"},
            ),
            tab="Data Drift",
        )
project.dashboard.add_panel(
             DashboardPanelPlot(
                title="Prediction drift",
                subtitle = """Drift in the prediction column ("class"), method: Jensen-Shannon distance""",
                size="half",
                values=[
                    PanelMetric(
                        legend="Drift score",
                        metric="ValueDrift",
                        metric_labels={"column": "class"} 
                    ),
                ],
                plot_params={"plot_type": "bar"},
            ),
            tab="Data Drift",
        )
```

This will result in the following Dashboard you'll be able to access in the Dashboard tab (left menu).

![](/images/examples/dashboard_quickstart.png)

For now, you will see only one datapoint, but as you add more Reports (e.g. daily or weekly), you'll be able to track the results over time.

# What's next?

- See available Evidently Metrics: [All Metric Table](/metrics/all_metrics)
- Understand how you can add conditional tests to your Reports: [Tests](/docs/library/tests).
- Explore options for Dashboard design: [Dashboards](/docs/platform/dashboard_add_panels)

<Info>
Alternatively, try `DataSummaryPreset` that will generate a summary of all columns in the dataset, and run auto-generated Tests to check for data quality and core descriptive stats.

```
report = Report([
    DataSummaryPreset() 
],
include_tests="True")
my_eval = report.run(eval_data_1, eval_data_2)
```
</Info>
